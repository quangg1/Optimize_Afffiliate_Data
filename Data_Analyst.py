import streamlit as st
import pandas as pd
import numpy as np
import io
import csv
from io import BytesIO
import zipfile
from difflib import SequenceMatcher
from concurrent.futures import ThreadPoolExecutor
from rapidfuzz import fuzz
import math

st.set_page_config(page_title="Optimize Affiliate Data", layout="wide")
st.title("üìä Optimize Affiliate Data")
def read_csv_safe(uploaded_file):
    try:
        return pd.read_csv(io.StringIO(uploaded_file.getvalue().decode('utf-8-sig')),
                           quoting=csv.QUOTE_MINIMAL, skipinitialspace=True,
                           engine='python', on_bad_lines='skip')
    except Exception as e:
        st.error(f"L·ªói ƒë·ªçc CSV: {e}")
    return pd.DataFrame()

def validate_columns(df):
    required_columns = ['Price', 'Link']
    if not set(required_columns).issubset(df.columns):
        st.error(f"Thi·∫øu c·ªôt b·∫Øt bu·ªôc: {', '.join(set(required_columns) - set(df.columns))}")
        return pd.DataFrame()
    df['Price'] = pd.to_numeric(df['Price'], errors='coerce')
    df = df[df['Link'].astype(str).str.match(r'^(http|https)://', na=False)]
    return df.dropna(subset=required_columns)

def preprocess_sold_column(df):
    if 'Sold' in df.columns:
        df['Sold'] = pd.to_numeric(
            df['Sold'].astype(str)
            .str.replace(r'(/th√°ng|l∆∞·ª£t b√°n|,)', '', regex=True)
            .str.replace('k', '000', regex=False),
            errors='coerce')
    return df

def process_group(group_df, input_ratio):
    # G·ªôp t·∫•t c·∫£ gi√° tr·ªã Sold th√†nh chu·ªói, c√°ch nhau d·∫•u ph·∫©y
    grouped = group_df.groupby("Link").agg({
        "Name": "first",  
        "Sold": lambda x: ",".join(map(str, x))  
    }).reset_index()
    
    # H√†m t√≠nh ƒë·ªô tƒÉng tr∆∞·ªüng
    def calculate_growth_rate(sold_str):
        sold_list = list(map(int, sold_str.split(',')))  
        if len(sold_list) < 2:  
            return -999  # N·∫øu ch·ªâ c√≥ 1 s·ªë, tr·∫£ v·ªÅ -999
        if len(sold_list) == 2:
            return 100  # N·∫øu ch·ªâ c√≥ 2 s·ªë, m·∫∑c ƒë·ªãnh tr·∫£ v·ªÅ 100
    
    # T√≠nh t·ªëc ƒë·ªô tƒÉng tr∆∞·ªüng theo c√¥ng th·ª©c t√πy ch·ªânh
        growth_rates = [((sold_list[i+1] - sold_list[i]) / (sold_list[i] - sold_list[i-1])) * 100 
                    for i in range(1, len(sold_list) - 1)]
    
        return np.mean(growth_rates)
 
    # T√≠nh Growth Rate
    grouped["Growth Rate"] = grouped["Sold"].apply(calculate_growth_rate)
    grouped["Sold"] = grouped["Sold"].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)
    # Chuy·ªÉn c·ªôt Sold th√†nh s·ªë nguy√™n n·∫øu ch·ªâ c√≥ 1 gi√° tr·ªã
    def process_sold(sold_str):
        sold_values = list(map(float, sold_str.split(',')))  # Chuy·ªÉn ƒë·ªïi th√†nh danh s√°ch s·ªë nguy√™n

        if len(sold_values) == 1:
            return sold_values[0]  # N·∫øu ch·ªâ c√≥ 1 gi√° tr·ªã, gi·ªØ nguy√™n
        else:
            return sold_values

    # L·ªçc tr√πng l·∫∑p d·ª±a v√†o t·ª∑ l·ªá gi·ªëng nhau c·ªßa t√™n s·∫£n ph·∫©m
    filtered_df = pd.DataFrame(columns=grouped.columns)  
    for _, row in grouped.iterrows():
        if not any(fuzz.ratio(row["Name"], other_name) >= input_ratio * 100 for other_name in filtered_df["Name"]):
            filtered_df = pd.concat([filtered_df, pd.DataFrame([row])], ignore_index=True)
# S·∫Øp x·∫øp d·ªØ li·ªáu
    above_1 = filtered_df[filtered_df["Growth Rate"] > -999].sort_values(by="Growth Rate", ascending=False)
# V·ªõi Growth Rate == -999, ƒë·∫£m b·∫£o "Sold" ƒë∆∞·ª£c so s√°nh d∆∞·ªõi d·∫°ng s·ªë
    filtered_df["Sold"] = filtered_df["Sold"].apply(pd.to_numeric, errors="coerce")
    equal_1 = filtered_df[filtered_df["Growth Rate"] == -999].copy()
    equal_1 = equal_1.sort_values(by="Sold", ascending=False)

# G·ªôp l·∫°i th√†nh k·∫øt qu·∫£ cu·ªëi c√πng

    return pd.concat([above_1, equal_1], ignore_index=True)

def remove_similar_keywords(df, input_ratio):
    results = []
    with ThreadPoolExecutor() as executor:
        futures = [executor.submit(process_group, df[df['Keyword'] == keyword], input_ratio) for keyword in df['Keyword'].unique()]
        for future in futures:
            result = future.result()
            if not result.empty:
                results.append(result)  # D√πng append thay v√¨ extend ƒë·ªÉ tr√°nh l·ªói

    return pd.concat(results, ignore_index=True) if results else pd.DataFrame()
def split_and_download_csv(above_df, equal_df, max_rows=500):
    total_rows = len(above_df) + len(equal_df)
    num_files = math.ceil(total_rows / max_rows)
    files = []
    
    # Chuy·ªÉn to√†n b·ªô d·ªØ li·ªáu c·ªßa c·ªôt c·∫ßn x·ª≠ l√Ω v·ªÅ chu·ªói ƒë·ªÉ tr√°nh l·ªói
    for col in above_df.columns:
        if col in equal_df.columns:
            above_df[col] = above_df[col].astype(str)
            equal_df[col] = equal_df[col].astype(str)
    
    # Chia ƒë·ªÅu d·ªØ li·ªáu v√†o c√°c file
    above_chunks = [above_df[i::num_files] for i in range(num_files)]
    equal_chunks = [equal_df[i::num_files] for i in range(num_files)]
    
    for i in range(num_files):
        # Gh√©p d·ªØ li·ªáu t·ª´ 2 nh√≥m
        combined_chunk = pd.concat([above_chunks[i], equal_chunks[i]], ignore_index=True)
        
        # L∆∞u v√†o file CSV ·∫£o
        csv_buffer = BytesIO()
        combined_chunk.to_csv(csv_buffer, index=False, encoding='utf-8-sig', quoting=1)  # quoting=1 ƒë·ªÉ tr√°nh l·ªói chuy·ªÉn ƒë·ªïi
        csv_buffer.seek(0)
        
        files.append((f"filtered_data_part_{i+1}.csv", csv_buffer))
    
    return files

def create_zip(files):
    zip_buffer = BytesIO()
    with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zip_file:
        for filename, file_buffer in files:
            zip_file.writestr(filename, file_buffer.getvalue())
    zip_buffer.seek(0)
    return zip_buffer


uploaded_files = st.file_uploader("Ch·ªçn nhi·ªÅu file CSV", type=["csv"], accept_multiple_files=True)

if uploaded_files:
    combined_df = pd.concat([
        validate_columns(preprocess_sold_column(read_csv_safe(file))) for file in uploaded_files
    ], ignore_index=True)

    if not combined_df.empty:
        st.subheader("üìå D·ªØ li·ªáu h·ª£p l·ªá ")
        st.dataframe(combined_df.sort_index(ascending=False), height=500,width=1500)

        number = st.number_input(label="T·ªâ l·ªá tr√πng l·∫∑p:", min_value=0.0, max_value=1.0, value=0.0, step=0.1)

        if st.button("üîç L·ªçc d·ªØ li·ªáu ") and number != 0:
            combined_df.dropna(subset=['Sold'], inplace=True)
            combined_df = remove_similar_keywords(combined_df, number)
            st.session_state.df_combined = combined_df
            above_df=st.session_state.df_combined[st.session_state.df_combined["Growth Rate"] > -999].sort_values(by="Growth Rate",ascending=False)
            equal_df=st.session_state.df_combined[st.session_state.df_combined["Growth Rate"] == -999].sort_values(by="Sold",ascending=False)
            st.subheader("üìå D·ªØ li·ªáu ƒë∆∞·ª£c c·∫≠p nh·∫≠t nhi·ªÅu ng√†y")
            st.dataframe(above_df, height=500, width=1500)
            st.subheader("ÔøΩÔøΩ D·ªØ li·ªáu ch·ªâ ch∆∞a c·∫≠p nh·∫≠t")
            st.dataframe(equal_df, height=500, width=1500)

            files = split_and_download_csv(above_df, equal_df)
            zip_buffer = create_zip(files)
    
            st.subheader("üì• T·∫£i xu·ªëng to√†n b·ªô CSV d∆∞·ªõi d·∫°ng ZIP")
            st.download_button(
            label="üìÅ T·∫£i xu·ªëng t·∫•t c·∫£ (ZIP)",
            data=zip_buffer,
            file_name="filtered_data.zip",
            mime="application/zip"
    )
    else:
        st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá sau khi ki·ªÉm tra.")
else:
    st.warning("Vui l√≤ng t·∫£i l√™n √≠t nh·∫•t m·ªôt file CSV.")
